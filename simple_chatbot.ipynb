{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hK0Fgo9i-Vzq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, random, unicodedata, re\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MHVBWVfx-Vzu"
   },
   "outputs": [],
   "source": [
    "cmdc_train = './cmdc/cmdc_train.txt'\n",
    "cmdc_dev = './cmdc/cmdc_dev.txt'\n",
    "cmdc_test = './cmdc/cmdc_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "21AdGTZQ-Vzx"
   },
   "source": [
    "## Initial data processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gg-2-6R4-Vzx"
   },
   "outputs": [],
   "source": [
    "def get_prompts_and_replies(filename):\n",
    "    '''loads the conversation data called filename (which should have prompts and replies on each line\n",
    "    separated by a tab and already tokenized), and returns parallel lists of prompts and replies'''\n",
    "    prompts = []\n",
    "    replies = []\n",
    "    with open(filename,encoding=\"utf-8\") as inF:\n",
    "        for line in inF:\n",
    "            prompt, reply = line.split('\\t')\n",
    "            prompts.append(prompt.split())\n",
    "            replies.append(reply.strip().split())\n",
    "    return prompts,replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9kpzdM9K-Vz0"
   },
   "outputs": [],
   "source": [
    "def get_longest_utterance(utterances):\n",
    "    '''get the longest utterance among a list of utterances'''\n",
    "    return max([len(utterance) for utterance in utterances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "36p4Z-GQ-Vz2"
   },
   "outputs": [],
   "source": [
    "def build_vocab_dicts(utterances):\n",
    "    '''given a list of utterances, build index dicts mapping tokens to ids and ids to tokens. Includes\n",
    "    special padding, start of sentence, end of sentence, and unknown tokens'''\n",
    "    vocab = set()\n",
    "    for utterance in utterances:\n",
    "        vocab.update(utterance)\n",
    "\n",
    "    vocab_to_idx = {'<pad>': 0, '<SOS>':1, '<EOS>':2,'<UNK>':3}\n",
    "    idx_to_vocab = {value:key for key,value in vocab_to_idx.items()}\n",
    "    for word in vocab:\n",
    "        idx_to_vocab[len(vocab_to_idx)] = word\n",
    "        vocab_to_idx[word] = len(vocab_to_idx)\n",
    "    return vocab_to_idx,idx_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w8ebYoKZ-Vz5"
   },
   "outputs": [],
   "source": [
    "def converttext2tensors(utterances, vocab_to_idx, longest_utterance):\n",
    "    '''given a list of lists of strings corresponding to utterances, converts each\n",
    "    utterance to a single 1d tensor of ids based on vocab_to_index, padded to longest_utterance'''\n",
    "    text_tensors =[]\n",
    "    for utterance in utterances:\n",
    "        tokens = np.array([vocab_to_idx.get(word,3) for word in utterance] + [2] + [0] * (longest_utterance - len(utterance)))\n",
    "        assert len(tokens) <= longest_utterance + 1\n",
    "        text_tensors.append(torch.tensor(tokens, dtype=torch.long))\n",
    "    return text_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qo3B_CA-Vz8"
   },
   "outputs": [],
   "source": [
    "class CBdataset(Dataset):\n",
    "    '''Builds our pytorch Dataset after preprocessing our text (vectorizing and padding)'''\n",
    "    def __init__(self, input_data, output_data):\n",
    "        self.input_data = input_data\n",
    "        self.output_data = output_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.input_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        target = self.output_data[index]\n",
    "        data_val = self.input_data[index]\n",
    "        return data_val,target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdUkFiTA-Vz-"
   },
   "source": [
    "## Building the neural chatboat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAsPPdR4-Vz-"
   },
   "outputs": [],
   "source": [
    "class WeightedSum(nn.Module):\n",
    "    '''converts a matrix of the form (token length X batch_size X hidden_dim) to one of shape\n",
    "    (batch_size, hidden_dim) by doing a weighted sum based on the similarity of each embedding\n",
    "    to weight_vector'''\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(WeightedSum,self).__init__()        \n",
    "        self.weight_vector = nn.Parameter(torch.randn(hidden_dim, requires_grad=True))\n",
    "    \n",
    "    def forward(self,question_embeddings):\n",
    "        question_embeddings = question_embeddings.permute(1,0,2)\n",
    "        weights = (question_embeddings @ self.weight_vector).softmax(dim=1)\n",
    "        weighted_question_embedding = (weights.unsqueeze(2)*question_embeddings).sum(axis=1)        \n",
    "        return weighted_question_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8weGkcpN-V0E"
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    '''The decoder part of a chatbot. Given an input_step of token ids for each batch, an last_hidden\n",
    "    state from the previous time step, and the representation of the prompt from the encoder, it\n",
    "    runs an RNN step, concatenates the output with the prompt embedding, and then applies two\n",
    "    hidden layers to predict the next word (token id)'''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.hidden_layer = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_output):\n",
    "        input_emb = self.embedding(input_step)\n",
    "        input_emb = self.embedding_dropout(input_emb)\n",
    "        rnn_out, hidden = self.gru(input_emb, last_hidden)\n",
    "        cat = torch.cat((rnn_out, encoder_output.unsqueeze(0)), dim = 2)\n",
    "        output = self.hidden_layer(cat)\n",
    "        output = torch.tanh(output)\n",
    "        output = self.out(output)\n",
    "        output = output.squeeze(0)\n",
    "        output = nn.functional.log_softmax(output, dim = 1)\n",
    "\n",
    "\n",
    "        return output, hidden\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "20f64e2aff49400181c2a1b711b8f25b",
      "c8b5da2f04c1473ea084e890b19ce9ef",
      "d9dddb19cf7f4a95ad717b2b61302efb",
      "7bbecf51f1c54e18a4cf6dac17996866",
      "9fba591df32c47f18b3c564ee39adad8",
      "ffb5433588004469be443ad41bc71073",
      "ae51e51c8ae6400f86704d34024c320e",
      "abfe879225154f8998351ba4600546c1",
      "3655224c187e4221b1c4f2bc8fd25296",
      "2072bea1f7da4c7da9374a1f515612ea",
      "31392163d5f440e59fb4519346423270",
      "5d5f4df8759e48daa6b74233807fe22c",
      "0016097053aa4bb9ac4bf9f0ee972098",
      "01bed17ab27149ae86c401931caa4678",
      "decd64ac7dfe4cfcaf1f783566ed00c8",
      "a59561679f6e4343aa37d2d81731359f",
      "bd65046c3fb74856bb45fc77c18eb5b2",
      "0df926a941c541cf8c4a19cb95b30d1a",
      "027173b27f1c4429bc835a9344d5338e",
      "f3d38e42dfbe460586ffa73ffce9c7ee",
      "277854c8d0c74de381c3edb6eb7fe37f",
      "2161ad91449c4e15b4ea772b15591e02",
      "e4f5b5b6b35647978dcd269a2f58ecd3",
      "58185a44e6734207bfd8f9c50e5ed66a"
     ]
    },
    "colab_type": "code",
    "id": "aZnm2rst-V0g",
    "outputId": "6a7581b6-3ddb-4098-a769-60c479de2fdb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f64e2aff49400181c2a1b711b8f25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3655224c187e4221b1c4f2bc8fd25296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=433, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd65046c3fb74856bb45fc77c18eb5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mwMv7o_V-V0i"
   },
   "source": [
    "BERT uses its own special wordpiece tokenization, and assumes sentences are initialized with the token `'[CLS]'` and that utterances are separated by the token `'[SEP]'`. The function provided below does the conversion to tensors for BERT, including padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TSifi9TK-V0j"
   },
   "outputs": [],
   "source": [
    "def pad_and_convert_to_tensor(utterances, longest):\n",
    "    '''Accepts variable lengths utterances and pads them to the same length'''    \n",
    "    new_list = []\n",
    "    for utterance in utterances:\n",
    "        new_list.append(torch.tensor(utterance + [0]*(longest -len(utterance))))\n",
    "    return new_list\n",
    "\n",
    "\n",
    "def get_bert_tensors(utterances):\n",
    "    '''Accepts a list of utterances, tokenizes them with BERTs tokenizer and converts wordpieces \n",
    "    to their integer IDs'''\n",
    "    indexed_utterances = []\n",
    "    longest = 0\n",
    "    for utterance in utterances:\n",
    "        utterance = '[CLS] ' + ' '.join(utterance) + ' [SEP]'\n",
    "        tokenized_utterance = bert_tokenizer.tokenize(utterance)\n",
    "        indexed_utterance = bert_tokenizer.convert_tokens_to_ids(tokenized_utterance)\n",
    "        longest = len(indexed_utterance) if len(indexed_utterance) > longest else longest\n",
    "        indexed_utterances.append(indexed_utterance)\n",
    "    return pad_and_convert_to_tensor(indexed_utterances,longest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnc7YWNe-V0m"
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    '''The encoder part of a chat bot. Given an input sequence corresponding to\n",
    "    (utterance length X batch_size) of token ids, converts them to embeddings, runs through\n",
    "    an RNN, does a weighted sum and returns a matrix (batch_size X embedding dim) of prompt\n",
    "    representations and the final hidden state of the RNN'''\n",
    "    def __init__(self, hidden_size, vocab_size, embedding_dim, n_layers=1, dropout=0, bert = False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bert = bert\n",
    "        if bert:\n",
    "            self.embedding = bert_model\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout), \n",
    "                          bidirectional=True)\n",
    "        self.hidden_layer = nn.Linear(hidden_dim*2, hidden_dim)\n",
    "        self.weighted_sum = WeightedSum(hidden_dim)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        if self.bert:\n",
    "            embedded, _ = self.embedding(input_ids=input_seq)\n",
    "        else:\n",
    "            embedded = self.embedding(input_seq)\n",
    "        input_lens = torch.sum(input_seq != 0, dim=0) # needed for packing\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lens, enforce_sorted=False)\n",
    "        rnn_out, hidden = self.gru(packed)\n",
    "        rnn_out, _ = nn.utils.rnn.pad_packed_sequence(rnn_out)\n",
    "        rnn_out = self.hidden_layer(rnn_out)\n",
    "        output = self.weighted_sum(rnn_out) \n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6IfbGsGE-V0p",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateReply(encoder, decoder, input_prompt, sample=False, bert = False):\n",
    "    if bert:\n",
    "        prompts_tensors = get_bert_tensors([input_prompt])\n",
    "    else:\n",
    "        prompts_tensors = converttext2tensors([input_prompt],vocab_to_idx,get_longest_utterance([input_prompt]))\n",
    "\n",
    "    prompts_tensors = prompts_tensors[0].unsqueeze(1)\n",
    "    #print(prompts_tensors.shape)\n",
    "\n",
    "    input_tensor = prompts_tensors.to(device)\n",
    "    encoder_out, encoder_hidden = encoder(input_tensor)\n",
    "    decoder_input = torch.LongTensor([[1 for _ in range(prompts_tensors.shape[1])]]) #1 is <SOS> idx\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    decoder_hidden = encoder_hidden[:decoder_n_layers]\n",
    "\n",
    "    output_words = []\n",
    "    max_length = 10\n",
    "\n",
    "    while len(output_words) < max_length:\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_out)\n",
    "        #print(decoder_output.shape) \n",
    "        if sample:\n",
    "            output_probs = np.exp(decoder_output.to('cpu').detach().numpy())[0]\n",
    "            output_idx = np.random.choice(decoder_output.shape[1], 1, p = output_probs)\n",
    "            decoder_input = torch.Tensor(output_idx).long().unsqueeze(1)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "        else:\n",
    "            output_idx = torch.argmax(decoder_output, dim = 1)\n",
    "            decoder_input = output_idx.unsqueeze(1)\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            output_idx = output_idx.item()\n",
    "\n",
    "        if bert:\n",
    "            word = bert_tokenizer.convert_ids_to_tokens(output_idx)\n",
    "        else:\n",
    "            word = idx_to_vocab[output_idx]\n",
    "\n",
    "        if word == '[CLS]':\n",
    "            continue\n",
    "        if word == '<EOS>' or word == '[SEP]':\n",
    "            break\n",
    "        # if word.startswith(\"##\"):\n",
    "        #     word = word[2:]\n",
    "        output_words.append(word)\n",
    "\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F-99hMc9-V0q"
   },
   "outputs": [],
   "source": [
    "def getUserInput(encoder, decoder,sample=False, bert = False):\n",
    "    '''Given a trained encoder and decoder for a chatbot, reads in user text, normalizes it \n",
    "    and passes it to the generateReply() function'''\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        # Get input sentence\n",
    "        input_sentence = input('> ')\n",
    "        # Check if it is quit case\n",
    "        if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "        # Normalize sentence\n",
    "        input_sentence = normalizeString(input_sentence)\n",
    "        # Generate reply\n",
    "        output_words = generateReply(encoder, decoder, input_sentence.split(),sample=sample, bert = bert)\n",
    "        # Format and print response sentence\n",
    "        output_words[:] = [x for x in output_words if not (x == '<EOS>' or x == '<pad>')]\n",
    "        print(chatbot_name + \":\", ' '.join(output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CUWtLh9ZUdVa",
    "outputId": "5ee598b5-ad63-4835-b7b6-cff1b47a4085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "hidden_dim = 100\n",
    "embedding_dim = 768\n",
    "batch_size = 128\n",
    "dropout = 0.1\n",
    "clip = 10\n",
    "decoder_n_layers = 1\n",
    "epochs = 30\n",
    "teacher_forcing_ratio = 0.8\n",
    "learning_rate = 1e-4\n",
    "decoder_learning_ratio = 5.0\n",
    "loss_function = nn.NLLLoss(ignore_index=0)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uRX9COaUeEZ"
   },
   "outputs": [],
   "source": [
    "prompts, replies = get_prompts_and_replies(cmdc_train)\n",
    "prompts_tensors = get_bert_tensors(prompts)\n",
    "replies_tensors = get_bert_tensors(replies)\n",
    "train_dataset = CBdataset(prompts_tensors, replies_tensors)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9n35UdyU9cz"
   },
   "outputs": [],
   "source": [
    "p_encoder = EncoderRNN(hidden_dim, bert_tokenizer.vocab_size, embedding_dim, bert=True)\n",
    "encoder_optimizer = optim.Adam(p_encoder.parameters(), lr=learning_rate)\n",
    "r_decoder = DecoderRNN(bert_tokenizer.vocab_size, embedding_dim, hidden_dim)\n",
    "decoder_optimizer = optim.Adam(r_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "mX_n9Ht_VIZL",
    "outputId": "f3be3651-ddb8-49cd-f877-bade815e6007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "After epoch 0 loss: 55.73053069612873\n",
      "Epoch: 1\n",
      "After epoch 1 loss: 42.43156409382227\n",
      "Epoch: 2\n",
      "After epoch 2 loss: 40.15135604232105\n",
      "Epoch: 3\n",
      "After epoch 3 loss: 38.98281636878626\n",
      "Epoch: 4\n",
      "After epoch 4 loss: 37.54163203547843\n",
      "Epoch: 5\n",
      "After epoch 5 loss: 36.125199953715004\n",
      "Epoch: 6\n",
      "After epoch 6 loss: 35.54246772462456\n",
      "Epoch: 7\n",
      "After epoch 7 loss: 35.35595742980046\n",
      "Epoch: 8\n",
      "After epoch 8 loss: 34.191769234576626\n",
      "Epoch: 9\n",
      "After epoch 9 loss: 33.44221929768425\n",
      "Epoch: 10\n",
      "After epoch 10 loss: 33.84090556434138\n",
      "Epoch: 11\n",
      "After epoch 11 loss: 31.80801658725264\n",
      "Epoch: 12\n",
      "After epoch 12 loss: 32.444529685214974\n",
      "Epoch: 13\n",
      "After epoch 13 loss: 32.23501539467579\n",
      "Epoch: 14\n",
      "After epoch 14 loss: 31.632749842174018\n",
      "Epoch: 15\n",
      "After epoch 15 loss: 31.365764157689032\n",
      "Epoch: 16\n",
      "After epoch 16 loss: 31.238405868188657\n",
      "Epoch: 17\n",
      "After epoch 17 loss: 30.55668385349103\n",
      "Epoch: 18\n",
      "After epoch 18 loss: 30.50053205063094\n",
      "Epoch: 19\n",
      "After epoch 19 loss: 30.635914275895303\n",
      "Epoch: 20\n",
      "After epoch 20 loss: 29.787514003355113\n",
      "Epoch: 21\n",
      "After epoch 21 loss: 29.629775023579004\n",
      "Epoch: 22\n",
      "After epoch 22 loss: 29.93684061724155\n",
      "Epoch: 23\n",
      "After epoch 23 loss: 29.828789231789052\n",
      "Epoch: 24\n",
      "After epoch 24 loss: 29.76103669731178\n",
      "Epoch: 25\n",
      "After epoch 25 loss: 28.5341948324175\n",
      "Epoch: 26\n",
      "After epoch 26 loss: 28.430446572564726\n",
      "Epoch: 27\n",
      "After epoch 27 loss: 27.933543769874383\n",
      "Epoch: 28\n",
      "After epoch 28 loss: 28.442804649694644\n",
      "Epoch: 29\n",
      "After epoch 29 loss: 27.34208955812217\n"
     ]
    }
   ],
   "source": [
    "# Ensure dropout layers are in train mode\n",
    "p_encoder.train()\n",
    "r_decoder.train()\n",
    "p_encoder.to(device)\n",
    "r_decoder.to(device)\n",
    "counter = 0\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    epoch_loss_history = []\n",
    "    for train_in, train_out in train_dataloader:\n",
    "        batch_loss = 0\n",
    "        train_in, train_out = train_in.t().to(device), train_out.t().to(device)\n",
    "        longest_utterance_in_batch = max(torch.sum(train_out != 0, dim=0))\n",
    "\n",
    "        # Pass through encoder\n",
    "        encoder_out, encoder_hidden = p_encoder(train_in)\n",
    "        \n",
    "        # Prep initial state of decoder\n",
    "        decoder_input = torch.LongTensor([[1 for _ in range(train_in.shape[1])]]) #1 is <SOS> idx\n",
    "        decoder_input = decoder_input.to(device)\n",
    "        decoder_hidden = encoder_hidden[:decoder_n_layers]\n",
    "\n",
    "        # Determine if we have teacher forcing\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        for t in range(longest_utterance_in_batch):   \n",
    "            decoder_output, decoder_hidden = r_decoder(decoder_input, decoder_hidden, encoder_out) \n",
    "            batch_loss += loss_function(decoder_output, train_out[t])\n",
    "            # implement the teacher forcing choice using the use_teacher_forcing boolean  \n",
    "            if use_teacher_forcing:\n",
    "                decoder_input = train_out[t].unsqueeze(0)\n",
    "            else:\n",
    "                decoder_input = torch.argmax(decoder_output, 1).unsqueeze(0)\n",
    "  \n",
    "        batch_loss.backward()\n",
    "        epoch_loss_history.append(batch_loss.item())\n",
    "        # Clip gradients: gradients are modified in place\n",
    "        _ = nn.utils.clip_grad_norm_(p_encoder.parameters(), clip)\n",
    "        _ = nn.utils.clip_grad_norm_(r_decoder.parameters(), clip)\n",
    "\n",
    "        # Adjust model weights\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        counter += 1\n",
    "        # if counter %20 == 0:\n",
    "        #     print('Processed:', counter*batch_size, 'of', len(train_dataset))\n",
    "            \n",
    "    print('After epoch', epoch, 'loss:', np.mean(epoch_loss_history))\n",
    "    counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4oBA43WW3xpL",
    "outputId": "0941170c-53d2-435f-b764-50cba5c72652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11482806908355375\n"
     ]
    }
   ],
   "source": [
    "def eval(encoder, decoder,dev_file):\n",
    "    '''evaluate a chatbot encoder/decoder by seeing how often given a prompt, it correctly predicts \n",
    "    the first word of the reply. Returns the accuracy'''\n",
    "    dev_prompts,dev_replies = get_prompts_and_replies(dev_file)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for i in range(len(dev_prompts)):\n",
    "        total += 1\n",
    "        prompt = dev_prompts[i]\n",
    "        gold_reply = dev_replies[i]\n",
    "        pred_reply = generateReply(p_encoder, r_decoder, prompt,bert=True)\n",
    "        if pred_reply and gold_reply and pred_reply[0] == gold_reply[0]:\n",
    "            correct += 1\n",
    "        \n",
    "    return correct/total\n",
    "\n",
    "score = eval(p_encoder,r_decoder,cmdc_dev)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0016097053aa4bb9ac4bf9f0ee972098": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "01bed17ab27149ae86c401931caa4678": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "027173b27f1c4429bc835a9344d5338e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2161ad91449c4e15b4ea772b15591e02",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_277854c8d0c74de381c3edb6eb7fe37f",
      "value": 440473133
     }
    },
    "0df926a941c541cf8c4a19cb95b30d1a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2072bea1f7da4c7da9374a1f515612ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20f64e2aff49400181c2a1b711b8f25b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d9dddb19cf7f4a95ad717b2b61302efb",
       "IPY_MODEL_7bbecf51f1c54e18a4cf6dac17996866"
      ],
      "layout": "IPY_MODEL_c8b5da2f04c1473ea084e890b19ce9ef"
     }
    },
    "2161ad91449c4e15b4ea772b15591e02": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "277854c8d0c74de381c3edb6eb7fe37f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "31392163d5f440e59fb4519346423270": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_01bed17ab27149ae86c401931caa4678",
      "max": 433,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0016097053aa4bb9ac4bf9f0ee972098",
      "value": 433
     }
    },
    "3655224c187e4221b1c4f2bc8fd25296": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31392163d5f440e59fb4519346423270",
       "IPY_MODEL_5d5f4df8759e48daa6b74233807fe22c"
      ],
      "layout": "IPY_MODEL_2072bea1f7da4c7da9374a1f515612ea"
     }
    },
    "58185a44e6734207bfd8f9c50e5ed66a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d5f4df8759e48daa6b74233807fe22c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a59561679f6e4343aa37d2d81731359f",
      "placeholder": "​",
      "style": "IPY_MODEL_decd64ac7dfe4cfcaf1f783566ed00c8",
      "value": " 433/433 [00:15&lt;00:00, 28.0B/s]"
     }
    },
    "7bbecf51f1c54e18a4cf6dac17996866": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abfe879225154f8998351ba4600546c1",
      "placeholder": "​",
      "style": "IPY_MODEL_ae51e51c8ae6400f86704d34024c320e",
      "value": " 232k/232k [00:00&lt;00:00, 733kB/s]"
     }
    },
    "9fba591df32c47f18b3c564ee39adad8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a59561679f6e4343aa37d2d81731359f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "abfe879225154f8998351ba4600546c1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae51e51c8ae6400f86704d34024c320e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd65046c3fb74856bb45fc77c18eb5b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_027173b27f1c4429bc835a9344d5338e",
       "IPY_MODEL_f3d38e42dfbe460586ffa73ffce9c7ee"
      ],
      "layout": "IPY_MODEL_0df926a941c541cf8c4a19cb95b30d1a"
     }
    },
    "c8b5da2f04c1473ea084e890b19ce9ef": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9dddb19cf7f4a95ad717b2b61302efb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffb5433588004469be443ad41bc71073",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9fba591df32c47f18b3c564ee39adad8",
      "value": 231508
     }
    },
    "decd64ac7dfe4cfcaf1f783566ed00c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4f5b5b6b35647978dcd269a2f58ecd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3d38e42dfbe460586ffa73ffce9c7ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_58185a44e6734207bfd8f9c50e5ed66a",
      "placeholder": "​",
      "style": "IPY_MODEL_e4f5b5b6b35647978dcd269a2f58ecd3",
      "value": " 440M/440M [00:14&lt;00:00, 30.0MB/s]"
     }
    },
    "ffb5433588004469be443ad41bc71073": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
